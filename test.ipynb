{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.  1.5 0.  0.  0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设你有一个包含 1 到 7 的 nparray\n",
    "arr = np.array([1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "# 输入你想要匹配的数字\n",
    "input_number = 3  # 假设输入数字为3\n",
    "\n",
    "# 生成一个新的array，满足条件的地方为1，其他地方为0\n",
    "result_array = np.where(arr == input_number, 1, 0)\n",
    "\n",
    "print(result_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "vgg = models.vgg16(pretrained=True)\n",
    "print(vgg.features)  # VGG16的卷积层部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# SUIM-Net model for underwater image segmentation\n",
    "# Paper: https://arxiv.org/pdf/2004.01241.pdf  \n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class MyUpSample2X(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, f_size=3):\n",
    "        super(MyUpSample2X, self).__init__()\n",
    "        self.up_sample=nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=f_size, stride=1, padding=f_size // 2)\n",
    "        self.bn = nn.BatchNorm2d(out_channels,momentum=0.8)\n",
    "\n",
    "    def forward(self, layer_input, skip_input):\n",
    "        u = self.up_sample(layer_input)\n",
    "        u = F.relu(self.bn(self.conv(u)))\n",
    "        u = torch.cat([u, skip_input], dim=1)\n",
    "        return u\n",
    "\n",
    "# Residual Skip Block (RSB)\n",
    "class RSB(nn.Module):\n",
    "    def __init__(self, filters, kernel_size, strides=1, skip=True):\n",
    "        super(RSB, self).__init__()\n",
    "        f1, f2, f3, f4 = filters\n",
    "        self.skip = skip\n",
    "        \n",
    "        # sub-block1\n",
    "        self.conv1 = nn.Conv2d(f1, f1, kernel_size=1, stride=strides)\n",
    "        self.bn1 = nn.BatchNorm2d(f1, momentum=0.8)\n",
    "        \n",
    "        # sub-block2\n",
    "        self.conv2 = nn.Conv2d(f1, f2, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.bn2 = nn.BatchNorm2d(f2, momentum=0.8)\n",
    "        \n",
    "        # sub-block3\n",
    "        self.conv3 = nn.Conv2d(f2, f3, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm2d(f3, momentum=0.8)\n",
    "\n",
    "        # optional skip connection\n",
    "        if not skip:\n",
    "            self.conv4 = nn.Conv2d(f1, f4, kernel_size=1, stride=strides)\n",
    "            self.bn4 = nn.BatchNorm2d(f4, momentum=0.8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        # Sub-block 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Sub-block 2\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "\n",
    "        # Sub-block 3\n",
    "        x = self.bn3(self.conv3(x))\n",
    "\n",
    "        if not self.skip:\n",
    "            shortcut = F.relu(self.bn4(self.conv4(shortcut)))\n",
    "\n",
    "        x = F.relu(x + shortcut)\n",
    "        return x\n",
    "\n",
    "# SUIM Encoder with RSB blocks\n",
    "class SuimEncoderRSB(nn.Module):\n",
    "    def __init__(self, channels=1):\n",
    "        super(SuimEncoderRSB, self).__init__()\n",
    "        \n",
    "        # Encoder block 1\n",
    "        self.conv1 = nn.Conv2d(channels, 64, kernel_size=5, stride=1)\n",
    "        \n",
    "        # Encoder block 2\n",
    "        self.bn1 = nn.BatchNorm2d(64, momentum=0.8)\n",
    "        self.rsb2a = RSB([64, 64, 128, 128], kernel_size=3, strides=2, skip=False)\n",
    "        self.rsb2b = RSB([64, 64, 128, 128], kernel_size=3, skip=True)\n",
    "        self.rsb2c = RSB([64, 64, 128, 128], kernel_size=3, skip=True)\n",
    "        \n",
    "        # Encoder block 3\n",
    "        self.rsb3a = RSB([128, 128, 256, 256], kernel_size=3, strides=2, skip=False)\n",
    "        self.rsb3b = RSB([128, 128, 256, 256], kernel_size=3, skip=True)\n",
    "        self.rsb3c = RSB([128, 128, 256, 256], kernel_size=3, skip=True)\n",
    "        self.rsb3d = RSB([128, 128, 256, 256], kernel_size=3, skip=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder block 1\n",
    "        enc_1 = self.conv1(x)\n",
    "\n",
    "        # Encoder block 2\n",
    "        x = F.relu(self.bn1(enc_1))\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        x = self.rsb2a(x)\n",
    "        x = self.rsb2b(x)\n",
    "        x = self.rsb2c(x)\n",
    "        enc_2 = x\n",
    "\n",
    "        # Encoder block 3\n",
    "        x = self.rsb3a(x)\n",
    "        x = self.rsb3b(x)\n",
    "        x = self.rsb3c(x)\n",
    "        x = self.rsb3d(x)\n",
    "        enc_3 = x\n",
    "\n",
    "        return [enc_1, enc_2, enc_3]\n",
    "\n",
    "\n",
    "class SuimDecoderRSB(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SuimDecoderRSB, self).__init__()\n",
    "\n",
    "        # Decoder block 1\n",
    "        self.conv1 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        # Decoder block 2\n",
    "        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        # Decoder block 3\n",
    "        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Final output layer\n",
    "        self.out_conv = nn.Conv2d(64, n_classes, kernel_size=3, padding=1)\n",
    "\n",
    "    def concat_skip(self, layer_input, skip_input, filters):\n",
    "        \"\"\" For concatenation of skip connections from the encoders. \"\"\"\n",
    "        u = F.relu(self.bn1(self.conv1(layer_input)))\n",
    "        u = torch.cat((u, skip_input), dim=1)  # Concatenation on channel axis\n",
    "        return u\n",
    "\n",
    "    def forward(self, enc_inputs):\n",
    "        enc_1, enc_2, enc_3 = enc_inputs\n",
    "        \n",
    "        # Decoder block 1\n",
    "        dec_1 = F.relu(self.bn1(self.conv1(enc_3)))\n",
    "        dec_1 = self.upsample1(dec_1)\n",
    "        \n",
    "        # Adjust dimensions using slicing for padding (similar to the Keras Lambda)\n",
    "        dec_1 = dec_1[:, :, :-2, :-2]  # Slice to match dimensions (cropping)\n",
    "        enc_2 = enc_2[:, :, :-1, :-1]  # Adjusting the dimensions of encoder 2\n",
    "        dec_1 = F.pad(dec_1, (1, 1, 1, 1))  # Zero padding to match dimensions\n",
    "        enc_2 = F.pad(enc_2, (1, 1, 1, 1))  # Zero padding for skip connection\n",
    "        \n",
    "        dec_1s = self.concat_skip(enc_2, dec_1, 256)\n",
    "\n",
    "        # Decoder block 2\n",
    "        dec_2 = F.relu(self.bn2(self.conv2(dec_1s)))\n",
    "        dec_2 = self.upsample2(dec_2)\n",
    "\n",
    "        dec_2s = F.relu(self.bn2(self.conv2(dec_2)))\n",
    "        dec_2s = self.upsample2(dec_2s)\n",
    "        \n",
    "        # Adjusting the dimensions of encoder 1\n",
    "        enc_1 = F.pad(enc_1, (2, 2, 2, 2))  # Zero padding to match dimensions\n",
    "\n",
    "        dec_2s = self.concat_skip(enc_1, dec_2s, 128)\n",
    "\n",
    "        # Decoder block 3\n",
    "        dec_3 = F.relu(self.bn3(self.conv3(dec_2s)))\n",
    "        dec_3s = F.relu(self.bn3(self.conv3(dec_3)))\n",
    "\n",
    "        # Final output layer with sigmoid activation\n",
    "        out = torch.sigmoid(self.out_conv(dec_3s))\n",
    "\n",
    "\n",
    "# Model class to combine encoder and decoder\n",
    "class SUIMNet(nn.Module):\n",
    "    def __init__(self, base='RSB', im_res=(320, 240), n_classes=5):\n",
    "        super(SUIMNet, self).__init__()\n",
    "        self.base = base\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        if self.base == 'RSB':\n",
    "            self.encoder = SuimEncoderRSB(channels=3)\n",
    "            self.decoder = SuimDecoderRSB(n_classes)\n",
    "\n",
    "        elif self.base == 'VGG':\n",
    "            # 使用预训练的 VGG16 模型\n",
    "            vgg = models.vgg16(pretrained=True)\n",
    "            self.encoder = vgg.features  # VGG16的卷积层部分\n",
    "\n",
    "            # 提取不同阶段的池化层\n",
    "            self.pool1 = nn.Identity()  # 第1池化层将由forward方法获取\n",
    "            self.pool2 = nn.Identity()  # 第2池化层将由forward方法获取\n",
    "            self.pool3 = nn.Identity()  # 第3池化层将由forward方法获取\n",
    "            self.pool4 = nn.Identity()  # 第4池化层将由forward方法获取\n",
    "\n",
    "            # 定义解码器部分\n",
    "            self.up1 = MyUpSample2X(512, 512)  # pool4 和 pool3\n",
    "            self.up2 = MyUpSample2X(512, 256)  # dec1 和 pool2\n",
    "            self.up3 = MyUpSample2X(256, 128)  # dec2 和 pool1\n",
    "            self.up4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            self.output_conv = nn.Conv2d(128, n_classes, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.base == 'RSB':\n",
    "            enc_outputs = self.encoder(x)\n",
    "            out = self.decoder(enc_outputs)\n",
    "        elif self.base == 'VGG':\n",
    "            pool1 = self.encoder[:5](x)  # 经过block1的卷积和池化\n",
    "            pool2 = self.encoder[5:10](pool1)  # 经过block2的卷积和池化\n",
    "            pool3 = self.encoder[10:17](pool2)  # 经过block3的卷积和池化\n",
    "            pool4 = self.encoder[17:24](pool3)  # 经过block4的卷积和池化\n",
    "            \n",
    "            dec1 = self.up1(pool4, pool3)\n",
    "            dec2 = self.up2(dec1, pool2)\n",
    "            dec3 = self.up3(dec2, pool1)\n",
    "            dec4 = self.up4(dec3)  # 上采样到原始分辨率\n",
    "            \n",
    "            out = self.output_conv(dec4)\n",
    "            \n",
    "            return torch.sigmoid(out)  # 使用 sigmoid 作为激活函数（适用于二元分割）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUIMNet(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (pool1): Identity()\n",
      "  (pool2): Identity()\n",
      "  (pool3): Identity()\n",
      "  (pool4): Identity()\n",
      "  (up1): MyUpSample2X(\n",
      "    (up_sample): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (up2): MyUpSample2X(\n",
      "    (up_sample): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (up3): MyUpSample2X(\n",
      "    (up_sample): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.8, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (up4): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "  (output_conv): Conv2d(128, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SUIMNet(base='VGG', im_res=(320, 240), n_classes=5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "train_dir = \"/home/liw324/code/data/SUIM_datasets/SUIM/train_val\"\n",
    "img_dir = f\"{train_dir}/images\"\n",
    "def get_paths(data_dir):\n",
    "    exts = ['*.png', '*.jpg', '*.jpeg', '*.bmp']\n",
    "    image_paths = []\n",
    "    for ext in exts:\n",
    "        image_paths.extend([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(ext)])\n",
    "    return sorted(image_paths)\n",
    "print(get_paths(img_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1525"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted(os.listdir(img_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1525"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "train_dir = \"/home/liw324/code/data/SUIM_datasets/SUIM/train_val\"\n",
    "mask_dir = f\"{train_dir}/masks\"\n",
    "len(sorted(os.listdir(mask_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "my_test = np.array([[1,2,3],\n",
    "                    [4,5,6],\n",
    "                    [7,8,9]])\n",
    "for i in my_test:\n",
    "    for j in i:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HD\n",
      "PF\n",
      "WR\n",
      "RO\n",
      "RI\n",
      "FV\n",
      "SR\n"
     ]
    }
   ],
   "source": [
    "mask_type={\"HD\":1,      # HD: Human divers\n",
    "           \"PF\":2,      # PF: Plants/sea-grass\n",
    "           \"WR\":3,      # WR: Wrecks/ruins\n",
    "           \"RO\":4,      # RO: Robots/instruments\n",
    "           \"RI\":5,      # RI: Reefs and invertebrates\n",
    "           \"FV\":6,      # FV: Fish and vertebrates\n",
    "           \"SR\":7       # SR: Sand/sea-floor (& rocks)\n",
    "           }  \n",
    "for i in (mask_type):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
